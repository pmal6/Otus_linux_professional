# Создаем ВМ с помощью предложенного Vagrantfile
# Установка zfs с вагранта не пошла
# вручную поставил все как в Vagranfile

# install zfs repo
#          yum install -y http://download.zfsonlinux.org/epel/zfs-release.el7_8.noarch.rpm
# import gpg key 
#          rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-zfsonlinux
# install DKMS style packages for correct work ZFS
#          yum install -y epel-release kernel-devel zfs
# change ZFS repo
#          yum-config-manager --disable zfs
#          yum-config-manager --enable zfs-kmod
#          yum install -y zfs
# Add kernel module zfs
#          modprobe zfs
# install wget
#          yum install -y wget

 
PS C:\Users\pmali> vagrant up
Bringing machine 'zfs' up with 'virtualbox' provider...
==> zfs: Importing base box 'centos/7'...
==> zfs: Matching MAC address for NAT networking...
==> zfs: Checking if box 'centos/7' version '2004.01' is up to date...
==> zfs: Setting the name of the VM: pmali_zfs_1701177214912_89705
==> zfs: Clearing any previously set network interfaces...
==> zfs: Preparing network interfaces based on configuration...
    zfs: Adapter 1: nat
==> zfs: Forwarding ports...
    zfs: 22 (guest) => 2222 (host) (adapter 1)
==> zfs: Running 'pre-boot' VM customizations...
==> zfs: Booting VM...
==> zfs: Waiting for machine to boot. This may take a few minutes...
    zfs: SSH address: 127.0.0.1:2222
    zfs: SSH username: vagrant
    zfs: SSH auth method: private key
    zfs:
    zfs: Vagrant insecure key detected. Vagrant will automatically replace
    zfs: this with a newly generated keypair for better security.
    zfs:
    zfs: Inserting generated public key within guest...
    zfs: Removing insecure key from the guest if it's present...
    zfs: Key inserted! Disconnecting and reconnecting using new SSH key...
==> zfs: Machine booted and ready!
==> zfs: Checking for guest additions in VM...
    zfs: No guest additions were detected on the base box for this VM! Guest
    zfs: additions are required for forwarded ports, shared folders, host only
    zfs: networking, and more. If SSH fails on this machine, please install
    zfs: the guest additions and repackage the box to continue.
    zfs:
    zfs: This is not an error message; everything may continue to work properly,
    zfs: in which case you may ignore this message.
==> zfs: Setting hostname...
==> zfs: Rsyncing folder: /cygdrive/c/Users/pmali/ => /vagrant
There was an error when attempting to rsync a synced folder.
Please inspect the error message below for more info.

Host path: /cygdrive/c/Users/pmali/
Guest path: /vagrant
Command: "rsync" "--verbose" "--archive" "--delete" "-z" "--copy-links" "--chmod=ugo=rwX" "--no-perms" "--no-owner" "--no-group" "--rsync-path" "sudo rsync" "-e" "ssh -p 2222 -o LogLevel=FATAL    -o IdentitiesOnly=yes -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i 'C:/Users/pmali/.vagrant/machines/zfs/virtualbox/private_key'" "--exclude" ".vagrant/" "/cygdrive/c/Users/pmali/" "vagrant@127.0.0.1:/vagrant"
Error: rsync: [sender] send_files failed to open "/cygdrive/c/Users/pmali/NTUSER.DAT": Device or resource busy (16)
rsync: [sender] send_files failed to open "/cygdrive/c/Users/pmali/ntuser.dat.LOG1": Device or resource busy (16)
rsync: [sender] send_files failed to open "/cygdrive/c/Users/pmali/ntuser.dat.LOG2": Device or resource busy (16)
Connection to 127.0.0.1 closed by remote host.
rsync: [sender] write error: Connection reset by peer (104)
rsync error: error in socket IO (code 10) at io.c(823) [sender=3.2.3]
PS C:\Users\pmali> vagrant ssh
PS C:\Users\pmali> vagrant ssh
Last login: Tue Nov 28 13:17:13 2023

[vagrant@zfs ~]$ whoami
vagrant
[vagrant@zfs ~]$ pwd
/home/vagrant

[vagrant@zfs ~]$ systemctl status sshd
● sshd.service - OpenSSH server daemon
   Loaded: loaded (/usr/lib/systemd/system/sshd.service; enabled; vendor preset: enabled)
   Active: active (running) since Tue 2023-11-28 13:14:37 UTC; 8min ago
     Docs: man:sshd(8)
           man:sshd_config(5)
 Main PID: 805 (sshd)
   CGroup: /system.slice/sshd.service
           └─805 /usr/sbin/sshd -D -u0

[vagrant@zfs ~]$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:4d:77:d3 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global noprefixroute dynamic eth0
       valid_lft 86340sec preferred_lft 86340sec
    inet6 2a03:d000:4325:871f:5054:ff:fe4d:77d3/64 scope global mngtmpaddr dynamic
       valid_lft forever preferred_lft forever
    inet6 fe80::5054:ff:fe4d:77d3/64 scope link
       valid_lft forever preferred_lft forever

[vagrant@zfs ~]$ ping 10.0.2.15
PING 10.0.2.15 (10.0.2.15) 56(84) bytes of data.
64 bytes from 10.0.2.15: icmp_seq=1 ttl=64 time=0.019 ms
64 bytes from 10.0.2.15: icmp_seq=2 ttl=64 time=0.054 ms
64 bytes from 10.0.2.15: icmp_seq=3 ttl=64 time=0.051 ms
64 bytes from 10.0.2.15: icmp_seq=4 ttl=64 time=0.053 ms
64 bytes from 10.0.2.15: icmp_seq=5 ttl=64 time=0.038 ms
64 bytes from 10.0.2.15: icmp_seq=6 ttl=64 time=0.080 ms
^C
--- 10.0.2.15 ping statistics ---
6 packets transmitted, 6 received, 0% packet loss, time 5001ms
rtt min/avg/max/mdev = 0.019/0.049/0.080/0.018 ms
[vagrant@zfs ~]$

## Смотрим список блочных устройств

[vagrant@zfs ~]$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda      8:0    0   40G  0 disk
└─sda1   8:1    0   40G  0 part /
sdb      8:16   0  512M  0 disk
sdc      8:32   0  512M  0 disk
sdd      8:48   0  512M  0 disk
sde      8:64   0  512M  0 disk
sdf      8:80   0  512M  0 disk
sdg      8:96   0  512M  0 disk
sdh      8:112  0  512M  0 disk
sdi      8:128  0  512M  0 disk
[vagrant@zfs ~]$

# Создаём пул из двух дисков в режиме RAID 1:
# +еще три пула

[root@zfs ~]# zpool create otus1 mirror /dev/sdb /dev/sdc
[root@zfs ~]# zpool create otus2 mirror /dev/sdd /dev/sde
[root@zfs ~]# zpool create otus3 mirror /dev/sdf /dev/sdg
[root@zfs ~]# zpool create otus4 mirror /dev/sdh /dev/sdi

# Смотрим информацию о пулах: zpool list


[root@zfs ~]# zpool list
NAME    SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
otus1   480M  91.5K   480M        -         -     0%     0%  1.00x    ONLINE  -
otus2   480M  91.5K   480M        -         -     0%     0%  1.00x    ONLINE  -
otus3   480M  91.5K   480M        -         -     0%     0%  1.00x    ONLINE  -
otus4   480M  91.5K   480M        -         -     0%     0%  1.00x    ONLINE  -
[root@zfs ~]#

# Команда zpool status показывает информацию о каждом диске, состоянии
# сканирования и об ошибках чтения, записи и совпадения хэш-сумм. Команда
# zpool list показывает информацию о размере пула, количеству занятого и
# свободного места, дедупликации и т.д.

# Добавим разные алгоритмы сжатия в каждую файловую систему:
# Алгоритм lzjb: zfs set compression=lzjb otus1
# Алгоритм lz4: zfs set compression=lz4 otus2
# Алгоритм gzip: zfs set compression=gzip-9 otus3
# Алгоритм zle: zfs set compression=zle otus4

[root@zfs ~]# zfs set compression=lzjb otus1
[root@zfs ~]# zfs set compression=lz4 otus2
[root@zfs ~]# zfs set compression=gzip-9 otus3
[root@zfs ~]# zfs set compression=zle otus4
[root@zfs ~]#

# Проверим, что все файловые системы имеют разные методы сжатия:

[root@zfs ~]# zfs get all | grep compression
otus1  compression           lzjb                   local
otus2  compression           lz4                    local
otus3  compression           gzip-9                 local
otus4  compression           zle                    local
[root@zfs ~]#

# Скачаем один и тот же текстовый файл во все пулы:

[root@zfs ~]# for i in {1..4}; do wget -P /otus$i https://gutenberg.org/cache/epub/2600/pg2600.converter.log; done
--2023-11-28 14:16:36--  https://gutenberg.org/cache/epub/2600/pg2600.converter.log
Resolving gutenberg.org (gutenberg.org)... 2610:28:3090:3000:0:bad:cafe:47, 152.19.134.47
Connecting to gutenberg.org (gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:443... failed: No route to host.
Connecting to gutenberg.org (gutenberg.org)|152.19.134.47|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 40988863 (39M) [text/plain]
Saving to: ‘/otus1/pg2600.converter.log’

100%[==============================================================================================================================================================>] 40,988,863  2.69MB/s   in 21s

2023-11-28 14:17:00 (1.85 MB/s) - ‘/otus1/pg2600.converter.log’ saved [40988863/40988863]

--2023-11-28 14:17:00--  https://gutenberg.org/cache/epub/2600/pg2600.converter.log
Resolving gutenberg.org (gutenberg.org)... 2610:28:3090:3000:0:bad:cafe:47, 152.19.134.47
Connecting to gutenberg.org (gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:443... failed: No route to host.
Connecting to gutenberg.org (gutenberg.org)|152.19.134.47|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 40988863 (39M) [text/plain]
Saving to: ‘/otus2/pg2600.converter.log’

100%[==============================================================================================================================================================>] 40,988,863  3.76MB/s   in 16s

2023-11-28 14:17:20 (2.50 MB/s) - ‘/otus2/pg2600.converter.log’ saved [40988863/40988863]

--2023-11-28 14:17:20--  https://gutenberg.org/cache/epub/2600/pg2600.converter.log
Resolving gutenberg.org (gutenberg.org)... 2610:28:3090:3000:0:bad:cafe:47, 152.19.134.47
Connecting to gutenberg.org (gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:443... failed: No route to host.
Connecting to gutenberg.org (gutenberg.org)|152.19.134.47|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 40988863 (39M) [text/plain]
Saving to: ‘/otus3/pg2600.converter.log’

100%[==============================================================================================================================================================>] 40,988,863  1.10MB/s   in 38s

2023-11-28 14:18:03 (1.02 MB/s) - ‘/otus3/pg2600.converter.log’ saved [40988863/40988863]

--2023-11-28 14:18:03--  https://gutenberg.org/cache/epub/2600/pg2600.converter.log
Resolving gutenberg.org (gutenberg.org)... 2610:28:3090:3000:0:bad:cafe:47, 152.19.134.47
Connecting to gutenberg.org (gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:443... failed: No route to host.
Connecting to gutenberg.org (gutenberg.org)|152.19.134.47|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 40988863 (39M) [text/plain]
Saving to: ‘/otus4/pg2600.converter.log’

100%[==============================================================================================================================================================>] 40,988,863  4.07MB/s   in 19s

2023-11-28 14:18:26 (2.04 MB/s) - ‘/otus4/pg2600.converter.log’ saved [40988863/40988863]

[root@zfs ~]#

# проверим, что файлы были скачаны во все пулы

[root@zfs ~]#  ls -l /otus*
/otus1:
total 22061
-rw-r--r--. 1 root root 40988863 Nov  2 08:10 pg2600.converter.log

/otus2:
total 17991
-rw-r--r--. 1 root root 40988863 Nov  2 08:10 pg2600.converter.log

/otus3:
total 10958
-rw-r--r--. 1 root root 40988863 Nov  2 08:10 pg2600.converter.log

/otus4:
total 40057
-rw-r--r--. 1 root root 40988863 Nov  2 08:10 pg2600.converter.log
[root@zfs ~]#

# Проверим, сколько места занимает один и тот же файл в разных пулах и
# проверим степень сжатия файлов:

[root@zfs ~]# zfs list
NAME    USED  AVAIL     REFER  MOUNTPOINT
otus1  21.6M   330M     21.6M  /otus1
otus2  17.7M   334M     17.6M  /otus2
otus3  10.8M   341M     10.7M  /otus3
otus4  39.2M   313M     39.1M  /otus4
[root@zfs ~]# zfs get all | grep compressratio | grep -v ref
otus1  compressratio         1.81x                  -
otus2  compressratio         2.22x                  -
otus3  compressratio         3.65x                  -
otus4  compressratio         1.00x                  -
[root@zfs ~]#

# Таким образом алгоритм gzip-9 самый эффективный по сжатию :

[root@zfs ~]# zfs get all | grep compression
otus1  compression           lzjb                   local
otus2  compression           lz4                    local
otus3  compression           gzip-9                 local
otus4  compression           zle                    local
[root@zfs ~]#

## ОПРЕДЕЛЕНИЕ НАСТРОЕК ПУЛА

# Скачиваем архив

[root@zfs ~]# wget -O archive.tar.gz --no-check-certificate 'https://drive.google.com/u/0/uc?id=1KRBNW33QWqbvbVHa3hLJivOAt60yukkg&e
> xport=download'
--2023-11-28 14:26:30--  https://drive.google.com/u/0/uc?id=1KRBNW33QWqbvbVHa3hLJivOAt60yukkg&e%0Axport=download
Resolving drive.google.com (drive.google.com)... 2a00:1450:400f:802::200e, 142.250.74.78
Connecting to drive.google.com (drive.google.com)|2a00:1450:400f:802::200e|:443... failed: No route to host.
Connecting to drive.google.com (drive.google.com)|142.250.74.78|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://drive.google.com/uc?id=1KRBNW33QWqbvbVHa3hLJivOAt60yukkg&e%0Axport=download [following]
--2023-11-28 14:26:32--  https://drive.google.com/uc?id=1KRBNW33QWqbvbVHa3hLJivOAt60yukkg&e%0Axport=download
Reusing existing connection to drive.google.com:443.
HTTP request sent, awaiting response... 303 See Other
Location: https://doc-0c-bo-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/uc866uhetq8j4akm8hnhgppb3gkuinpa/1701181575000/16189157874053420687/*/1KRBNW33QWqbvbVHa3hLJivOAt60yukkg?uuid=9c936517-b6e1-4f7c-a7e1-e1152c0a892e [following]
Warning: wildcards not supported in HTTP.
--2023-11-28 14:26:36--  https://doc-0c-bo-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/uc866uhetq8j4akm8hnhgppb3gkuinpa/1701181575000/16189157874053420687/*/1KRBNW33QWqbvbVHa3hLJivOAt60yukkg?uuid=9c936517-b6e1-4f7c-a7e1-e1152c0a892e
Resolving doc-0c-bo-docs.googleusercontent.com (doc-0c-bo-docs.googleusercontent.com)... 2a00:1450:400f:80d::2001, 216.58.211.1
Connecting to doc-0c-bo-docs.googleusercontent.com (doc-0c-bo-docs.googleusercontent.com)|2a00:1450:400f:80d::2001|:443... failed: No route to host.
Connecting to doc-0c-bo-docs.googleusercontent.com (doc-0c-bo-docs.googleusercontent.com)|216.58.211.1|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 7275140 (6.9M) [application/x-gzip]
Saving to: ‘archive.tar.gz’

100%[==============================================================================================================================================================>] 7,275,140   2.25MB/s   in 3.1s

2023-11-28 14:26:43 (2.25 MB/s) - ‘archive.tar.gz’ saved [7275140/7275140]

[root@zfs ~]#
# загрузили файл archive.tar.gz

[root@zfs ~]# ls -l
total 7124
-rw-------. 1 root root    5570 Apr 30  2020 anaconda-ks.cfg
-rw-r--r--. 1 root root 7275140 May 15  2020 archive.tar.gz
-rw-------. 1 root root    5300 Apr 30  2020 original-ks.cfg
[root@zfs ~]#

# Разархивируем его

[root@zfs ~]# tar -xzvf archive.tar.gz
zpoolexport/
zpoolexport/filea
zpoolexport/fileb
[root@zfs ~]#

# Проверим, возможно ли импортировать данный каталог в пул. Обзор команды zpool import
# zpool import [-D] [-d dir|device]
# Перечисляет пулы, доступные для импорта. Если опция -d не указана, эта команда ищет устройства в /dev.
# Опцию -d можно указать несколько раз, при этом поиск будет осуществляться во всех каталогах.
# Если устройство является частью экспортированного пула, эта команда отображает сводную информацию о пуле с именем пула,
# числовым идентификатором, а также макетом vdev и текущим состоянием устройства для каждого устройства или файла.
# Уничтоженные пулы, пулы, которые ранее были уничтожены с помощью команды zpool destroy, не отображаются в списке, если не указана опция -D.

[root@zfs ~]# zpool import -d zpoolexport/
   pool: otus
     id: 6554193320433390805
  state: ONLINE
 action: The pool can be imported using its name or numeric identifier.
 config:

        otus                         ONLINE
          mirror-0                   ONLINE
            /root/zpoolexport/filea  ONLINE
            /root/zpoolexport/fileb  ONLINE
[root@zfs ~]#

# Сделаем импорт данного пула к нам в ОС:

[root@zfs ~]# zpool import -d zpoolexport/ otus


[root@zfs ~]# zpool status
  pool: otus
 state: ONLINE
  scan: none requested
config:

        NAME                         STATE     READ WRITE CKSUM
        otus                         ONLINE       0     0     0
          mirror-0                   ONLINE       0     0     0
            /root/zpoolexport/filea  ONLINE       0     0     0
            /root/zpoolexport/fileb  ONLINE       0     0     0

errors: No known data errors

  pool: otus1
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        otus1       ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            sdb     ONLINE       0     0     0
            sdc     ONLINE       0     0     0

errors: No known data errors

  pool: otus2
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        otus2       ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            sdd     ONLINE       0     0     0
            sde     ONLINE       0     0     0

errors: No known data errors

  pool: otus3
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        otus3       ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            sdf     ONLINE       0     0     0
            sdg     ONLINE       0     0     0

errors: No known data errors

  pool: otus4
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        otus4       ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            sdh     ONLINE       0     0     0
            sdi     ONLINE       0     0     0

errors: No known data errors
[root@zfs ~]#

#у ZFS ПУЛА МАССА ПАРАМЕТРОВ
#Запрос сразу всех параметром файловой системы: zfs get all otus

[root@zfs ~]# zfs get all otus
NAME  PROPERTY              VALUE                  SOURCE
otus  type                  filesystem             -
otus  creation              Fri May 15  4:00 2020  -
otus  used                  2.04M                  -
otus  available             350M                   -
otus  referenced            24K                    -
otus  compressratio         1.00x                  -
otus  mounted               yes                    -
otus  quota                 none                   default
otus  reservation           none                   default
otus  recordsize            128K                   local
otus  mountpoint            /otus                  default
otus  sharenfs              off                    default
otus  checksum              sha256                 local
otus  compression           zle                    local
otus  atime                 on                     default
otus  devices               on                     default
otus  exec                  on                     default
otus  setuid                on                     default
otus  readonly              off                    default
otus  zoned                 off                    default
otus  snapdir               hidden                 default
otus  aclinherit            restricted             default
otus  createtxg             1                      -
otus  canmount              on                     default
otus  xattr                 on                     default
otus  copies                1                      default
otus  version               5                      -
otus  utf8only              off                    -
otus  normalization         none                   -
otus  casesensitivity       sensitive              -
otus  vscan                 off                    default
otus  nbmand                off                    default
otus  sharesmb              off                    default
otus  refquota              none                   default
otus  refreservation        none                   default
otus  guid                  14592242904030363272   -
otus  primarycache          all                    default
otus  secondarycache        all                    default
otus  usedbysnapshots       0B                     -
otus  usedbydataset         24K                    -
otus  usedbychildren        2.01M                  -
otus  usedbyrefreservation  0B                     -
otus  logbias               latency                default
otus  objsetid              54                     -
otus  dedup                 off                    default
otus  mlslabel              none                   default
otus  sync                  standard               default
otus  dnodesize             legacy                 default
otus  refcompressratio      1.00x                  -
otus  written               24K                    -
otus  logicalused           1020K                  -
otus  logicalreferenced     12K                    -
otus  volmode               default                default
otus  filesystem_limit      none                   default
otus  snapshot_limit        none                   default
otus  filesystem_count      none                   default
otus  snapshot_count        none                   default
otus  snapdev               hidden                 default
otus  acltype               off                    default
otus  context               none                   default
otus  fscontext             none                   default
otus  defcontext            none                   default
otus  rootcontext           none                   default
otus  relatime              off                    default
otus  redundant_metadata    all                    default
otus  overlay               off                    default
otus  encryption            off                    default
otus  keylocation           none                   default
otus  keyformat             none                   default
otus  pbkdf2iters           0                      default
otus  special_small_blocks  0                      default
[root@zfs ~]#

# C помощью команды grep можно уточнить конкретный параметр, например:

[root@zfs ~]# zfs get available otus
NAME  PROPERTY   VALUE  SOURCE
otus  available  350M   -
[root@zfs ~]#

[root@zfs ~]# zfs get readonly otus
NAME  PROPERTY  VALUE   SOURCE
otus  readonly  off     default
[root@zfs ~]#


[root@zfs ~]# zfs get recordsize otus
NAME  PROPERTY    VALUE    SOURCE
otus  recordsize  128K     local
[root@zfs ~]#

[root@zfs ~]# zfs get compression otus
NAME  PROPERTY     VALUE     SOURCE
otus  compression  zle       local
[root@zfs ~]#

[root@zfs ~]# zfs get checksum otus
NAME  PROPERTY  VALUE      SOURCE
otus  checksum  sha256     local
[root@zfs ~]#

## Работа со снапшотом, поиск сообщения от преподавателя

## Загрузим файл

[root@zfs ~]# wget -O otus_task2.file --no-check-certificate "https://drive.google.com/u/0/uc?id=1gH8gCL9y7Nd5Ti3IRmplZPF1XjzxeRAG&export=download"
--2023-11-28 15:56:29--  https://drive.google.com/u/0/uc?id=1gH8gCL9y7Nd5Ti3IRmplZPF1XjzxeRAG&export=download
Resolving drive.google.com (drive.google.com)... 142.250.74.174, 2a00:1450:400f:805::200e
Connecting to drive.google.com (drive.google.com)|142.250.74.174|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://drive.google.com/uc?id=1gH8gCL9y7Nd5Ti3IRmplZPF1XjzxeRAG&export=download [following]
--2023-11-28 15:56:30--  https://drive.google.com/uc?id=1gH8gCL9y7Nd5Ti3IRmplZPF1XjzxeRAG&export=download
Reusing existing connection to drive.google.com:443.
HTTP request sent, awaiting response... 303 See Other
Location: https://doc-00-bo-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/eb89gnqjokd4m9fpnrkbgm2h3h9lpvsp/1701240975000/16189157874053420687/*/1gH8gCL9y7Nd5Ti3IRmplZPF1XjzxeRAG?e=download&uuid=91491d01-cd4c-4c47-ab24-a899ec26c421 [following]
Warning: wildcards not supported in HTTP.
--2023-11-28 15:56:35--  https://doc-00-bo-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/eb89gnqjokd4m9fpnrkbgm2h3h9lpvsp/1701240975000/16189157874053420687/*/1gH8gCL9y7Nd5Ti3IRmplZPF1XjzxeRAG?e=download&uuid=91491d01-cd4c-4c47-ab24-a899ec26c421
Resolving doc-00-bo-docs.googleusercontent.com (doc-00-bo-docs.googleusercontent.com)... 216.58.207.193, 2a00:1450:400f:80b::2001
Connecting to doc-00-bo-docs.googleusercontent.com (doc-00-bo-docs.googleusercontent.com)|216.58.207.193|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 5432736 (5.2M) [application/octet-stream]
Saving to: ‘otus_task2.file’

100%[==============================================================================================================================================================>] 5,432,736   3.65MB/s   in 1.4s

2023-11-28 15:56:37 (3.65 MB/s) - ‘otus_task2.file’ saved [5432736/5432736]

[root@zfs ~]#

[root@zfs ~]# ll
total 12432
-rw-------. 1 root root    5570 Apr 30  2020 anaconda-ks.cfg
-rw-r--r--. 1 root root 7275140 May 15  2020 archive.tar.gz
-rw-------. 1 root root    5300 Apr 30  2020 original-ks.cfg
-rw-r--r--. 1 root root 5432736 May 15  2020 otus_task2.file
drwxr-xr-x. 2 root root      32 May 15  2020 zpoolexport

# Восстановим файловую систему из снапшота:

[root@zfs ~]# zfs receive otus/test@today < otus_task2.file
[root@zfs ~]#
[root@zfs ~]# zfs list
NAME             USED  AVAIL     REFER  MOUNTPOINT
otus            4.93M   347M       25K  /otus
otus/hometask2  1.88M   347M     1.88M  /otus/hometask2
otus/test       2.83M   347M     2.83M  /otus/test
otus1           21.6M   330M     21.6M  /otus1
otus2           17.7M   334M     17.6M  /otus2
otus3           10.8M   341M     10.7M  /otus3
otus4           39.2M   313M     39.1M  /otus4
[root@zfs ~]#

# Находим secret_message в /otus/test/task1/file_mess/

[root@zfs file_mess]# find /otus/test -name "secret_message"
/otus/test/task1/file_mess/secret_message
[root@zfs file_mess]#

[root@zfs ~]# cd /otus/test/
[root@zfs test]# ll
total 2590
-rw-r--r--. 1 root    root          0 May 15  2020 10M.file
-rw-r--r--. 1 root    root     727040 May 15  2020 cinderella.tar
-rw-r--r--. 1 root    root         65 May 15  2020 for_examaple.txt
-rw-r--r--. 1 root    root          0 May 15  2020 homework4.txt
-rw-r--r--. 1 root    root     309987 May 15  2020 Limbo.txt
-rw-r--r--. 1 root    root     509836 May 15  2020 Moby_Dick.txt
drwxr-xr-x. 3 vagrant vagrant       4 Dec 18  2017 task1
-rw-r--r--. 1 root    root    1209374 May  6  2016 War_and_Peace.txt
-rw-r--r--. 1 root    root     398635 May 15  2020 world.sql

[root@zfs test]# cd task1/

[root@zfs task1]# ll
total 240
drwxr-xr-x. 2 vagrant vagrant 1386 May 15  2020 file_mess
-rw-r--r--. 1 vagrant vagrant 1803 Dec 18  2017 README

[root@zfs task1]# cd file_mess/

[root@zfs file_mess]# cat secret_message

https://github.com/sindresorhus/awesome

[root@zfs file_mess]#

